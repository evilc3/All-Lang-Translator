{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data \n",
    "\n",
    "# Spanish - to - english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"data/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = io.open(path,encoding = \"UTF-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = file.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Go.', 'Ve.'], ['Go.', 'Vete.'], ['Go.', 'Vaya.']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[w for w in l.split(\"\\t\")[:2]] for l in lines[0:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  2, 33,  3,  0,  0,  0],\n",
       "       [ 1,  1,  2, 33,  3,  1,  1,  1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                            [[1,1,2,33,3],\n",
    "                             [1,1,2,33,3,1,1,1]]\n",
    "                             ,padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hindi - English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"data/hin.txt\"\n",
    "file = io.open(path,encoding = \"UTF-8\").read()\n",
    "lines = file.split(\"\\n\")\n",
    "words = [[w for w in l.split(\"\\t\")[:2]] for l in lines[0:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['मैं', 'ठीक', 'हूँ।']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[10][1].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    \n",
    "      return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "          if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w,):\n",
    "\n",
    "      w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "      \n",
    "      w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "      w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "      # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "      w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "      w = w.strip()\n",
    "\n",
    "      # adding a start and an end token to the sentence\n",
    "      # so that the model know when to start and stop predicting.\n",
    "      w = '<start> ' + w + ' <end>'\n",
    "      return w\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input <start> ! <end>\n",
      "target: <start> wow ! <end>\n",
      "input <start> ! <end>\n",
      "target: <start> help ! <end>\n",
      "input <start> . <end>\n",
      "target: <start> jump . <end>\n",
      "input <start> . <end>\n",
      "target: <start> jump . <end>\n",
      "input <start> . <end>\n",
      "target: <start> jump . <end>\n",
      "input <start>  <end>\n",
      "target: <start> hello ! <end>\n",
      "input <start>  <end>\n",
      "target: <start> hello ! <end>\n",
      "input <start> ! <end>\n",
      "target: <start> cheers ! <end>\n",
      "input <start> ! <end>\n",
      "target: <start> cheers ! <end>\n",
      "input <start>  <end> <start>  <end> <start> ? <end>\n",
      "target: <start> got <end> <start> it ? <end>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tar,inp in words[0:10]:\n",
    "    \n",
    "    print('input',' '.join([preprocess_sentence(w) for w in inp.split()]))\n",
    "\n",
    "    print('target:',' '.join([preprocess_sentence(w) for w in tar.split()]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Will need to chnage preprocess function to suport hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input <start> वाह! <end>\n",
      "target: <start> wow ! <end>\n",
      "input <start> बचाओ! <end>\n",
      "target: <start> help ! <end>\n",
      "input <start> उछलो. <end>\n",
      "target: <start> jump . <end>\n",
      "input <start> कदो. <end>\n",
      "target: <start> jump . <end>\n",
      "input <start> छलाग. <end>\n",
      "target: <start> jump . <end>\n",
      "input <start> नमसत। <end>\n",
      "target: <start> hello ! <end>\n",
      "input <start> नमसकार। <end>\n",
      "target: <start> hello ! <end>\n",
      "input <start> वाह-वाह! <end>\n",
      "target: <start> cheers ! <end>\n",
      "input <start> चियरस! <end>\n",
      "target: <start> cheers ! <end>\n",
      "input <start> समझ कि नही? <end>\n",
      "target: <start> got it ? <end>\n"
     ]
    }
   ],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    \n",
    "      return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "          if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w,regex = True):\n",
    "\n",
    "      w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "      if regex:\n",
    "    \n",
    "          w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "          w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "\n",
    "          # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "          w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "      w = w.strip()\n",
    "\n",
    "      # adding a start and an end token to the sentence\n",
    "      # so that the model know when to start and stop predicting.\n",
    "      w = '<start> ' + w + ' <end>'\n",
    "      return w\n",
    "\n",
    "\n",
    "for tar,inp in words[0:10]:\n",
    "    \n",
    "    print('input',' '.join([preprocess_sentence(inp,regex = False)]))\n",
    "\n",
    "    print('target:',' '.join([preprocess_sentence(tar)]))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no. of times 'you' occur in  ref sent is '1'\n",
      " no. of times 'must' occur in  ref sent is '0'\n",
      " no. of times 'be' occur in  ref sent is '0'\n",
      " no. of times 'punished' occur in  ref sent is '0'\n",
      " no. of times '.' occur in  ref sent is '1'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perfect match \n",
    "\n",
    "# ref =  ['i', 'm', 'very', 'optimistic', '.']\n",
    "# mt  = ['i', 'm', 'very', 'optimistic', '.']\n",
    "\n",
    "reference =   ['you', 'need', 'to', 'shut', 'up', '.']\n",
    "mt  =  ['you', 'must', 'be', 'punished', '.']\n",
    "\n",
    "\n",
    "# can have more than 1 ref \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_counts(x):\n",
    "\n",
    "  return Counter(x)\n",
    "\n",
    "\n",
    "def get_striped_counts(mt,ref):\n",
    "\n",
    "  # check if the translated word is present in the refrence word.\n",
    "\n",
    "  '''\n",
    "  number of t\n",
    "\n",
    "  '''\n",
    "\n",
    "  mt_dict = get_counts(mt)\n",
    "\n",
    "  ref_dict = get_counts(ref)    \n",
    "      \n",
    "     \n",
    "    \n",
    "\n",
    "  #ref_dict = get_counts(ref)\n",
    "  # common words between ref and mt \n",
    "\n",
    "  num = 0\n",
    "  den = 0\n",
    "\n",
    "\n",
    "  for words in mt_dict.keys():\n",
    "      #number of times mt_words occur in ref_words\n",
    "        \n",
    "         print(f' no. of times \\'{words}\\' occur in  ref sent is \\'{ref_dict[words]}\\'')\n",
    "     \n",
    "         num += ref_dict[words]\n",
    "\n",
    "         den += mt_dict[words]\n",
    "  '''\n",
    "  calculate : no. of times word occurs in ref / no. of times word occurs in mt only\n",
    "  '''\n",
    "  return num/den \n",
    "\n",
    "# get_counts(mt).keys()\n",
    "get_striped_counts(mt,ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['this', 'is', 'a', 'test'],['this','is','test']]\n",
    "candidate = ['this', 'is', 'a','test']\n",
    "score = sentence_bleu(reference, candidate,weights = (1,0,0,0))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no. of times 'this' occur in  ref sent is '1'\n",
      " no. of times 'is' occur in  ref sent is '1'\n",
      " no. of times 'a' occur in  ref sent is '1'\n",
      " no. of times 'test' occur in  ref sent is '1'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_striped_counts(candidate,reference[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
